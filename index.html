<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <meta name="description"
        content="treamingT2V: Consistent, Dynamic, and Extendable Long Video Generation from Text.">
  <meta name="keywords" content="StreamingT2V">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <title>StreamingT2V: Consistent, Dynamic, and Extendable Long Video Generation from Text</title>

  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=G-PYVRSFMDRL"></script>
  <script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
  <script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.0/MathJax.js?config=TeX-MML-AM_CHTML"></script>
  <script id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
  <style>
    .inline-equation {
      display: inline-block;
      vertical-align: middle;
    }
  </style>

  <script>
    window.dataLayer = window.dataLayer || [];

    function gtag() {
      dataLayer.push(arguments);
    }

    gtag('js', new Date());

    gtag('config', 'G-PYVRSFMDRL');
  </script>

  <link href="https://fonts.googleapis.com/css?family=Google+Sans|Noto+Sans|Castoro"
        rel="stylesheet">

  <link rel="stylesheet" href="./static/css/bulma.min.css">
  <link rel="stylesheet" href="./static/css/bulma-carousel.min.css">
  <link rel="stylesheet" href="./static/css/bulma-slider.min.css">
  <link rel="stylesheet" href="./static/css/fontawesome.all.min.css">
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css">
  <link rel="stylesheet" href="./static/css/index.css">
  <link rel="icon" href="./static/images/favicon.png">

  <style>  
    table {  
      font-family: arial, sans-serif;  
      border-collapse: collapse;  
      width: 100%;  
    }  
      
    td, th {  
      border: 2px solid #F1F4F5;  
      text-align: left;  
      padding: 8px;  
    }  
    
    tr:nth-child(3n - 1) {  
      background-color: #F1F4F5;  
    }  

    tr:nth-child(3n) {  
      border: 2px solid #FFFFFF;
    }  
  </style>

  <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>
  <script defer src="./static/js/fontawesome.all.min.js"></script>
  <script src="./static/js/bulma-carousel.min.js"></script>
  <script src="./static/js/bulma-slider.min.js"></script>
  <script src="./static/js/index.js"></script>
</head>
<body>

<section class="hero">
  <div class="hero-body">
    <div class="container is-max-desktop">
      <div class="columns is-centered">
        <div class="column has-text-centered">
          <h1 class="title is-1 publication-title">StreamingT2V: <br> Consistent, Dynamic, and Extendable Long Video Generation from Text</h1>
          <div class="is-size-5 publication-authors">
            <span class="author-block">
              <a href="https://www.linkedin.com/in/dr-ing-roberto-henschel-6aa1ba176">Roberto Henschel</a><sup>1*</sup>,</span>
            </span>
            <span class="author-block">
              <a href="https://levon-kh.github.io/">Levon Khachatryan</a><sup>1*</sup>,</span>
            <span class="author-block">
              <a href="https://www.linkedin.com/in/daniil-hayrapetyan-375b05149/">Daniil Hayrapetyan</a><sup>1*</sup>,</span>
            </span><br>
            <span class="author-block">
              <a href="https://www.linkedin.com/in/hayk-poghosyan-793b97198/">Hayk Poghosyan</a><sup>1</sup>,</span>
            <span class="author-block">
              <a href="https://www.linkedin.com/in/vtadevosian">Vahram Tadevosyan</a><sup>1</sup>,</span>
            <span class="author-block">
              <a href="https://www.ece.utexas.edu/people/faculty/atlas-wang">Zhangyang Wang</a><sup>1,2</sup>,
            </span>
            <span class="author-block">
              <a href="https://www.linkedin.com/in/shant-navasardyan-1302aa149">Shant Navasardyan</a><sup>1</sup>,
            </span>
            <span class="author-block">
              <a href="https://www.humphreyshi.com">Humphrey Shi</a><sup>1,3</sup>
            </span>
          </div>

          <div class="is-size-5 publication-authors">
            <span class="author-block"><sup>1</sup>Picsart AI Resarch (PAIR),</span>
            <span class="author-block"><sup>2</sup>UT Austin</span>
            <span class="author-block"><sup>3</sup>SHI Labs @ Georgia Tech, Oregon & UIUC</span>
          </div>
          <div class="is-size-5 publication-authors">
            <span class="author-block"><sup>*</sup>Equal Contribution</span>
          </div>

          <div class="column has-text-centered">
            <div class="publication-links">
              <!-- PDF Link. -->
              <!-- <span class="link-block">
                <a href=""
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="fas fa-file-pdf"></i>
                  </span>
                  <span>Paper</span>
                </a>
              </span> -->
              <span class="link-block">
                <!-- FIX THE LINK -->
                <a href="https://github.com/Picsart-AI-Research/StreamingT2V/blob/main/__assets__/github/pdf/StreamingT2V.pdf"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="ai ai-arxiv"></i>
                  </span>
                  <span>arXiv</span>
                </a>
              </span>
              <!-- Video Link. -->
              <!-- <span class="link-block">
                <a href=""
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="fab fa-youtube"></i>
                  </span>
                  <span>Video</span>
                </a>
              </span> -->
              <!-- Code Link. -->
              <span class="link-block">
                <a href="https://github.com/Picsart-AI-Research/StreamingT2V"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="fab fa-github"></i>
                  </span>
                  <span>Code</span>
                  </a>
              </span>
              <!-- Demo Link. -->
<!--               <span class="link-block">
                <a href="https://huggingface.co/spaces/PAIR/StreamingT2V"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                    <img src="./static/images/hf.png" alt="Button Image">
                  </span>
                  <span>Demo</span>
                  </a> -->
            </div>

          </div>
        </div>
      </div>
    </div>
  </div>
</section>

<section class="hero teaser">
  <div class="container is-max-desktop">
    <div class="hero-body">
      <img id="teaser" autoplay muted loop playsinline height="100%" src="./static/images/teaser.png" style="width:100%;height:100%;">
      <p class="subtitle has-text-centered">
        <b><span class="dnerf">StreamingT2V</span></b> is an advanced autoregressive technique that enables the creation of long videos featuring rich motion dynamics without any stagnation. It ensures temporal consistency throughout the video, aligns closely with the descriptive text, and maintains high frame-level image quality. Our demonstrations include successful examples of videos up to <b>1200 frames, spanning 2 minutes</b>, and can be extended for even longer durations. Importantly, the effectiveness of StreamingT2V is not limited by the specific Text2Video model used, indicating that improvements in base models could yield even higher-quality videos.
        <!-- The overall pipeline of <span class="dnerf">StreamingT2V</span>: In the <span class="dnerf">Short T2V Stage</span>, the first 16-frame chunk is synthesized by a text-to-video model. In the <span class="dnerf">Streaming T2V Stage</span>, the new content for further frames are autoregressively generated. Finally, in the <span class="dnerf">Streaming Refinement Stage</span>, the generated long video (80 frames or more) is autoregressively enhanced by applying a high-resolution text-to-short-video model, equipped with our randomized blending approach. -->
      </p>
    </div>
  </div>
</section>


<section class="section">
  <div class="container is-max-desktop">
    <!-- Abstract. -->
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3">Abstract</h2>
        <div class="content has-text-justified">
          <p>
            Text-to-video diffusion models enable the generation of high-quality videos that follow text instructions, making it easy to create diverse and individual content.
            However, existing approaches mostly focus on high-quality short video generation (typically 16 or 24 frames), ending up with hard-cuts when naively extended to the case of long video synthesis. To overcome these limitations, we introduce <b><span class="dnerf">StreamingT2V</span></b>, an autoregressive approach for long video generation of <b>80, 240, 600, 1200</b> or more frames with smooth transitions.
            The key components are:
            (i) a short-term memory block called conditional attention module (CAM), which conditions the current generation on the features extracted from the previous chunk via an attentional mechanism, leading to consistent chunk transitions, 
            (ii) a long-term memory block called appearance preservation module, which extracts high-level scene and object features from the first video chunk to prevent the model from forgetting the initial scene,  and (iii) a randomized blending approach that enables to apply a video enhancer autoregressively for infinitely long videos without inconsistencies between chunks.   
            Experiments show that <span class="dnerf">StreamingT2V</span> generates high motion amount. In contrast, all competing image-to-video methods are prone to video stagnation when applied naively in an autoregressive manner.
            Thus, we propose with <span class="dnerf">StreamingT2V</span> a high-quality seamless text-to-long video generator that outperforms competitors with consistency and motion. 
          </p>
        </div>
      </div>
    </div>
    <!--/ Abstract. -->

    <!-- Paper video. -->
    <!-- <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3">Video</h2>
        <div class="publication-video">
          <iframe src="https://www.youtube.com/embed/MrKrnHhk8IA?rel=0&amp;showinfo=0"
                  frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe>
        </div>
      </div>
    </div> -->

    <!-- <video id="teaser" autoplay muted loop playsinline height="100%"> -->
    <video id="replay-video"
                 controls
                 muted
                 preload
                 playsinline
                 height="100%">
      <source src="./static/videos/StreamingT2V.mp4"
              type="video/mp4">
    </video>

    <!--/ Paper video. -->
  </div>
</section>


<section class="section" id="Method">
  <div class="container is-max-desktop content">
    <h2 class="title">Method</h2>
    <section class="hero method">
      <div class="container is-max-desktop">
        <div class="hero-body">
          <img id="method" autoplay muted loop playsinline height="100%" src="./static/images/t2v-v8.png" style="width:100%;height:80%;">
          <p>
            Method overview: <span class="dnerf">StreamingT2V</span> extends a video diffusion model (VDM) by the conditional attention module (CAM) as short-term memory,  and the appearance preservation module (APM) as long-term memory. CAM conditions the VDM on the previous chunk using a frame encoder <span class="inline-equation">$$\mathcal{E}_{\mathrm{cond}}$$</span>.<br> The attentional mechanism of CAM leads to smooth transitions between chunks and videos with high motion amount at the same time. APM extracts from an anchor frame high-level image features and injects it to the text cross-attentions of the VDM. APM helps to preserve object/scene features across the autogressive video generation.
          </p>
        </div>
      </div>
    </section>
  </div>
</section>


<section class="section" id="Results">
  <div class="container is-max-desktop content">
    <h2 class="title">Results</h2>
    <section class="hero method">
    <div class="container is-max-desktop">
    <div class="hero-body">  
    <h3 class="title">SECTION 1</h3>
    <table class="center">
      <tr><td></td><td></td><td></td><td></td></tr>
      <tr>
        <td><img src="./static/images/vip2p_frame2_up_right.gif" raw=true></td>
        <td><img src="./static/images/vip2p_frame2_up_right.gif"></td>
        <td><img src="./static/images/vip2p_frame2_up_right.gif"></td>              
        <td><img src="./static/images/vip2p_frame2_up_right.gif"></td>
      </tr>
      <tr>
        <td width=25% style="text-align:center;">"Prompt Template"</td>
        <td width=25% style="text-align:center;">"Prompt Template</td>
        <td width=25% style="text-align:center;">"Prompt Template"</td>
        <td width=25% style="text-align:center;">"Prompt Template"</td>
      </tr>
      <tr><td></td><td></td><td></td><td></td></tr>
    </table>
    <br>
    <h3 class="title">SECTION 2</h3>
    <table class="center">
      <tr><td></td><td></td><td></td><td></td></tr>
      <tr>
        <td><img src="./static/images/vip2p_frame2_up_right.gif" raw=true></td>
        <td><img src="./static/images/vip2p_frame2_up_right.gif"></td>
        <td><img src="./static/images/vip2p_frame2_up_right.gif"></td>              
        <td><img src="./static/images/vip2p_frame2_up_right.gif"></td>
      </tr>
      <tr>
        <td width=25% style="text-align:center;">"Prompt Template"</td>
        <td width=25% style="text-align:center;">"Prompt Template</td>
        <td width=25% style="text-align:center;">"Prompt Template"</td>
        <td width=25% style="text-align:center;">"Prompt Template"</td>
      </tr>
      <tr><td></td><td></td><td></td><td></td></tr>
    </table>
    <br>
    <h3 class="title">SECTION 3</h3>
    <table class="center">
      <tr><td></td><td></td><td></td></tr>
      <tr>
        <td><img src="./static/images/vip2p_frame2_up_right.gif" raw=true><img src="./static/images/vip2p_frame2_up_right.gif"></td>
        <td><img src="./static/images/vip2p_frame2_up_right.gif" raw=true><img src="./static/images/vip2p_frame2_up_right.gif"></td>
        <td><img src="./static/images/vip2p_frame2_up_right.gif" raw=true><img src="./static/images/vip2p_frame2_up_right.gif"></td>
      </tr>
      <tr>
        <td width=25% style="text-align:center;">"Prompt Template"</td>
        <td width=25% style="text-align:center;">"Prompt Template"</td>
          <td width=25% style="text-align:center;">"Prompt Template"</td>
      </tr>
    </table>
  </div></div></section>
  </div>
</section>


<section class="section" id='RelatedLinks'>
  <div class="container is-max-desktop content">
    <h2 class="title">Related Links</h2>

    <ul>
      <li><a href=""> RELATED LINK TEMPLATE</a></li>
    </ul>
    <!-- <div class="content has-text-justified">
      <p>
        There's a lot of excellent work that was introduced around the same time as ours.
      </p>
      <p>
        <a href="https://arxiv.org/abs/2104.09125">Progressive Encoding for Neural Optimization</a> introduces an idea similar to our windowed position encoding for coarse-to-fine optimization.
      </p>
      <p>
        <a href="https://www.albertpumarola.com/research/D-NeRF/index.html">D-NeRF</a> and <a href="https://gvv.mpi-inf.mpg.de/projects/nonrigid_nerf/">NR-NeRF</a>
        both use deformation fields to model non-rigid scenes.
      </p>
      <p>
        Some works model videos with a NeRF by directly modulating the density, such as <a href="https://video-nerf.github.io/">Video-NeRF</a>, <a href="https://www.cs.cornell.edu/~zl548/NSFF/">NSFF</a>, and <a href="https://neural-3d-video.github.io/">DyNeRF</a>
      </p>
      <p>
        There are probably many more by the time you are reading this. Check out <a href="https://dellaert.github.io/NeRF/">Frank Dellart's survey on recent NeRF papers</a>, and <a href="https://github.com/yenchenlin/awesome-NeRF">Yen-Chen Lin's curated list of NeRF papers</a>.
      </p>
    </div> -->
  </div></section>
  </div>
</section>


<section class="section" id="BibTeX">
  <div class="container is-max-desktop content">
    <h2 class="title">BibTeX</h2>
    <p> If you use our work in your research, please cite our publication: </p>
    <pre><code>@article{streamingt2v,
    title={StreamingT2V: Consistent and Extendable Long Video Generation from Text},
    author={Henschel, Roberto and Khachatryan, Levon and Hayrapetyan, Daniil and Poghosyan, Hayk and Tadevosyan, Vahram and Wang, Zhangyang and Navasardyan, Shant and Shi, Humphrey},
    journal={arXiv preprint arXiv:{ARXIV_ID}},
    year={2024}
  }
    </code></pre>
  </div>
</section>

<footer class="footer">
  <div class="container">
    <div class="content has-text-centered">
      <!-- FIX THE LINK -->
      <a class="icon-link" href="https://github.com/Picsart-AI-Research/StreamingT2V/blob/main/__assets__/github/pdf/StreamingT2V.pdf">
        <i class="fas fa-file-pdf"></i>
      </a>
      <!-- <a class="icon-link" href="https://github.com/keunhong" class="external-link" disabled> -->
      <a class="icon-link" href="https://github.com/Picsart-AI-Research/StreamingT2V" class="external-link" disabled>
        <i class="fab fa-github"></i>
      </a>
    </div>
    <div class="columns is-centered">
      <div class="column is-8">
        <div class="content">
          <p>
            Website adapted from the following <a href="https://github.com/nerfies/nerfies.github.io">template</a>.
          </p>
        </div>
      </div>
    </div>
  </div>
</footer>

</body>
</html>
